{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Problemas del libro Reinforcement Learning: An Introduction de Richard S. Sutton and Andrew G. Barto. La biblia del aprendizaje por refuerzo. Publicado bajo un acuerdo de open access, por lo que el libro está disponible en forma gratuita por los autores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy Gridworld\n",
    "\n",
    "En este problema un agente debe recorrer una cuadrícula donde existen corrientes de viento que lo empujan en una dirección por lo cual se debe encontrar el camino optimo para llegar a la meta.\n",
    "\n",
    "![title](a2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entorno\n",
    "El agente se encuentra en una cuadrícula de 10x7 cuadros. En las cuadriculas se encuentran corrientes de aire la cual dificultan el camino del agente.\n",
    "\n",
    "El agente comienza en la coordenada $(1, 4)$ y debe dirigirse lo más pronto posible a la meta ubicada en la coordenada $(8, 4)$. En cualquier coordenada puede decidir moverse en las 4 direcciones cardinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORLD_HEIGHT = 7\n",
    "WORLD_WIDTH = 10\n",
    "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "\n",
    "ACTION_UP = 1\n",
    "ACTION_DOWN = 2\n",
    "ACTION_LEFT = 3\n",
    "ACTION_RIGHT = 4\n",
    "\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.5\n",
    "REWARD = -1.0\n",
    "\n",
    "START  = [4, 1]\n",
    "GOAL  = [4, 8]\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step(state, action)\n",
    "    i, j = state\n",
    "    \n",
    "    if action == ACTION_UP\n",
    "        return [max(i - 1 - WIND[j], 1), j]\n",
    "    elseif action == ACTION_DOWN\n",
    "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT), 1), j]\n",
    "    elseif action == ACTION_LEFT\n",
    "        return [max(i - WIND[j], 1), max(j - 1, 1)]\n",
    "    elseif action == ACTION_RIGHT\n",
    "        return [max(i - WIND[j], 1), min(j + 1, WORLD_WIDTH)]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function episode(q_value)\n",
    "    time_ = 0\n",
    "    state = START \n",
    "\n",
    "    if rand(Binomial(1, EPSILON)) == 1\n",
    "        action = rand(ACTIONS)\n",
    "    else\n",
    "        values_ = q_value[state[1], state[2], :]\n",
    "        action = rand([action_ for (action_, value_) in enumerate(values_) if value_ == maximum(values_)])\n",
    "    end\n",
    "\n",
    "    while state != GOAL \n",
    "        next_state = step(state, action)\n",
    "        \n",
    "        if rand(Binomial(1, EPSILON)) == 1\n",
    "            next_action = rand(ACTIONS)\n",
    "        else\n",
    "            values_ = q_value[next_state[1], next_state[2], :]\n",
    "            next_action = rand([action_ for (action_, value_) in enumerate(values_) if value_ == maximum(values_)])\n",
    "        end\n",
    "        \n",
    "        # Sarsa update\n",
    "        q_value[state[1], state[2], action] += \n",
    "            ALPHA * (reward + q_value[next_state[1], next_state[2], next_action] -\n",
    "                         q_value[state[1], state[2], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        time_ += 1\n",
    "    end\n",
    "    \n",
    "    return time_\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Any,1}:\n",
       "  1002\n",
       "  1091\n",
       "  1296\n",
       "  1753\n",
       "  1779\n",
       "  1915\n",
       "  2005\n",
       "  2096\n",
       "  2228\n",
       "  2380\n",
       "  2410\n",
       "  2482\n",
       "  2537\n",
       "     ⋮\n",
       " 14705\n",
       " 14726\n",
       " 14743\n",
       " 14760\n",
       " 14780\n",
       " 14800\n",
       " 14820\n",
       " 14837\n",
       " 14853\n",
       " 14871\n",
       " 14888\n",
       " 14938"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value = zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
    "episode_limit = 500\n",
    "\n",
    "steps = []\n",
    "ep = 0\n",
    "\n",
    "while ep < episode_limit\n",
    "    append!(steps, episode(q_value))\n",
    "    ep += 1\n",
    "end\n",
    "\n",
    "steps = accumulate(+, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      "Any['→', '→', '→', '→', '→', '→', '→', '→', '→', '↓']\n",
      "Any['↑', '→', '→', '→', '→', '→', '→', '→', '→', '↓']\n",
      "Any['↑', '→', '→', '→', '→', '→', '→', '←', '→', '↓']\n",
      "Any['→', '→', '→', '→', '→', '→', '→', 'G', '→', '↓']\n",
      "Any['→', '→', '→', '→', '→', '→', '↑', '↓', '←', '←']\n",
      "Any['→', '→', '→', '→', '→', '↑', '↑', '↓', '←', '←']\n",
      "Any['→', '→', '→', '→', '↑', '↑', '↑', '↑', '↓', '↓']\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = []\n",
    "for i in range(1, WORLD_HEIGHT)\n",
    "    push!(optimal_policy, [])\n",
    "    \n",
    "    for j in range(1, WORLD_WIDTH)\n",
    "        if [i, j] == GOAL \n",
    "            append!(optimal_policy[end],\"G\")\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        bestAction = argmax(q_value[i, j, :])\n",
    "        \n",
    "        if bestAction == ACTION_UP\n",
    "            append!(optimal_policy[end], \"↑\")\n",
    "        elseif bestAction == ACTION_DOWN\n",
    "            append!(optimal_policy[end], \"↓\")\n",
    "        elseif bestAction == ACTION_LEFT\n",
    "            append!(optimal_policy[end], \"←\")\n",
    "        elseif bestAction == ACTION_RIGHT\n",
    "            append!(optimal_policy[end], \"→\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Optimal policy:\")\n",
    "for row in optimal_policy\n",
    "    println(row)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
