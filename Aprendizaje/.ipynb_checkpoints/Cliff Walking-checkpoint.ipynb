{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking\n",
    "El agente debe llegar de un punto A a un punto B sin caer en las casillas inferiores, ya que deberia volver al punto A.\n",
    "![title](a.png)\n",
    "\n",
    "El agente debe aprender a navegar una cuadrícula de 12x4, en donde las coordenadas horizontales $(2, 1)$, $(3, 1)$, $(4, 1)$, $(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(11, 1)$ representan un abismo, en el que el agente es penalizado con una recompensa de -100 y debe regresar a la coordenada inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORLD_HEIGHT = 4\n",
    "WORLD_WIDTH = 12\n",
    "START = [4, 1]\n",
    "GOAL = [4, 12];\n",
    "\n",
    "EPSILON = 0.5\n",
    "ALPHA = 0.5\n",
    "GAMMA = 1\n",
    "\n",
    "ACTION_UP = 1\n",
    "ACTION_DOWN = 2\n",
    "ACTION_LEFT = 3\n",
    "ACTION_RIGHT = 4\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step(state, action)\n",
    "    i, j = state\n",
    "    if action == ACTION_UP\n",
    "        next_state = [max(i - 1, 1), j]\n",
    "    elseif action == ACTION_LEFT\n",
    "        next_state = [i, max(j - 1, 1)]\n",
    "    elseif action == ACTION_RIGHT\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH)]\n",
    "    elseif action == ACTION_DOWN\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT), j]\n",
    "    else\n",
    "        return false\n",
    "    end\n",
    "    \n",
    "    reward = -1\n",
    "    if (action == ACTION_DOWN && i == 3 && 2 <= j && j <= 11) ||\n",
    "        (action == ACTION_RIGHT && state == START)\n",
    "        reward = -100\n",
    "        next_state = START\n",
    "    end\n",
    "    return next_state, reward\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_action (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function choose_action(state, q_value)\n",
    "    if rand(1)[1] < EPSILON\n",
    "        return ACTIONS[rand(1:4)]\n",
    "    else\n",
    "        values_ = q_value[state[1], state[2],:]\n",
    "        action = []\n",
    "        for i in enumerate(values_)\n",
    "            action_,value_ = i\n",
    "            if value_ == maximum(values_)\n",
    "                push!(action,action_)\n",
    "            end\n",
    "        end\n",
    "        action = action[rand(1:length(action))]\n",
    "        return action\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarsa (generic function with 3 methods)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sarsa(q_value, expected = false, step_size = ALPHA)\n",
    "    state = START\n",
    "    action = choose_action(state, q_value)\n",
    "    rewards = 0.0\n",
    "    while state != GOAL\n",
    "        next_state, reward = step(state, action)\n",
    "        next_action = choose_action(next_state,q_value)\n",
    "        rewards += reward\n",
    "        if !expected\n",
    "            target = q_value[next_state[1], next_state[2], next_action]\n",
    "        else\n",
    "            target = 0.0\n",
    "            q_next = q_value[next_state[1], next_state[2], :]\n",
    "            best_actions = transpose(hcat(nonzero(q_next == np.max(q_next))...))\n",
    "            for action_ in ACTIONS\n",
    "                if action_ in best_actions\n",
    "                    target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[1], next_state[2], action_]\n",
    "                else\n",
    "                    target += EPSILON / len(ACTIONS) * q_value[next_state[1], next_state[2], action_]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        target = target * GAMMA\n",
    "        q_value[state[1], state[2], action] += step_size * (reward + target - q_value[state[1], state[2], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    end\n",
    "    return rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 2 methods)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function q_learning(q_value, step_size = ALPHA)\n",
    "    state = START\n",
    "    rewards = 0.0\n",
    "    while state != GOAL\n",
    "        action = choose_action(state, q_value)\n",
    "        next_state, reward = step(state, action)\n",
    "        rewards += reward\n",
    "\n",
    "        q_value[state[1], state[2], action] += step_size * (\n",
    "                reward + GAMMA * maximum(q_value[next_state[1], next_state[2], :]) -\n",
    "                q_value[state[1], state[2], action])\n",
    "        state = next_state\n",
    "    end\n",
    "    return rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_optimal_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function print_optimal_policy(q_value)\n",
    "    optimal_policy = []\n",
    "    for i in range(1, stop=WORLD_HEIGHT)\n",
    "        push!(optimal_policy,[])\n",
    "        for j in range(1, stop=WORLD_WIDTH)\n",
    "            if [i, j] == GOAL\n",
    "                append!(optimal_policy[end], 'G')\n",
    "                continue\n",
    "            end\n",
    "            bestAction = argmax(q_value[i, j, :])\n",
    "            if bestAction == ACTION_UP\n",
    "                append!(optimal_policy[end], '↑')\n",
    "            elseif bestAction == ACTION_DOWN\n",
    "                append!(optimal_policy[end], '↓')\n",
    "            elseif bestAction == ACTION_LEFT\n",
    "                append!(optimal_policy[end], '←')\n",
    "            elseif bestAction == ACTION_RIGHT\n",
    "                append!(optimal_policy[end], '→')\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    for row in optimal_policy\n",
    "        println(row)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " -4025.94\n",
       " -1495.1 \n",
       " -1152.7 \n",
       "  -656.16\n",
       "  -840.78\n",
       "  -617.62\n",
       "  -591.22\n",
       "  -571.6 \n",
       "  -506.76\n",
       "  -467.28\n",
       "  -442.64\n",
       "  -328.62\n",
       "  -503.78\n",
       "     ⋮   \n",
       "  -829.1 \n",
       "  -643.06\n",
       "  -664.6 \n",
       "  -669.88\n",
       "  -692.22\n",
       "  -654.78\n",
       "  -661.84\n",
       "  -786.46\n",
       "  -597.9 \n",
       "  -857.92\n",
       "  -954.24\n",
       "  -781.16"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = 500\n",
    "runs = 50\n",
    "\n",
    "rewards_sarsa = zeros(episodes)\n",
    "rewards_q_learning = zeros(episodes)\n",
    "q_sarsa = zeros((world_height, world_width, 4))\n",
    "q_q_learning = deepcopy(q_sarsa)\n",
    "\n",
    "for r ∈ range(1, runs)\n",
    "    q_sarsa = zeros((world_height, world_width, 4))\n",
    "    q_q_learning = deepcopy(q_sarsa)\n",
    "\n",
    "    for i ∈ range(1, episodes)\n",
    "        rewards_sarsa[i] += sarsa(q_sarsa)\n",
    "        rewards_q_learning[i] += q_learning(q_q_learning)\n",
    "    end\n",
    "end\n",
    "\n",
    "rewards_sarsa /= runs\n",
    "rewards_q_learning /= runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados del aprendizaje "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarsa:\n",
      "Any['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓']\n",
      "Any['↑', '↑', '↑', '←', '↑', '→', '↑', '←', '→', '↑', '→', '↓']\n",
      "Any['↑', '←', '↑', '↑', '↑', '→', '↑', '↑', '↑', '↑', '→', '↓']\n",
      "Any['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', 'G']\n",
      "Q-Learning:\n",
      "Any['↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓']\n",
      "Any['↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓']\n",
      "Any['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓']\n",
      "Any['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', 'G']\n"
     ]
    }
   ],
   "source": [
    "println(\"Sarsa:\")\n",
    "print_optimal_policy(q_sarsa)\n",
    "println(\"Q-Learning:\")\n",
    "print_optimal_policy(q_q_learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
